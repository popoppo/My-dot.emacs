==============================================
8.3 The nuts and bolts of resource discovery
==============================================
8.3 資源発見のポイント
==============================================

      The simplistic approach to networked applications is that you hardcode the locations
      of all the resources on the network. When things are added, relocated, or removed
      (for any number of reasons including scaling up, reorganizing, replacing failed
      instances, or code upgrades), this hardcoded configuration must be altered manually.
      If you’ve been a good software engineer, you probably have all these settings in a
      configuration file or database. If you’ve been lazy and entered it in the source code, you
      have to edit and recompile your modules. In either case, this manual process is both
      slow and error prone, and it often leads to confusing setups over time as things are
      moved around and reconfigured.

ネットワークアプリケーションへの単純化されたアプローチは、ネットワーク上の全てのリソースのロケーションをハードコードしてしまうことだ。リソースが追加、再配置または削除されたら(スケールアップ、再構成、不正なインスタンスの置き換えまたはアップデートを含む多くの理由により)このハードコードによる設定は手動で変更されなければならない。もしあなたが優れたソフトウェアエンジニアならば、それらの設定を設定ファイルかDBに持つだろう。もしあなたが怠惰で、それをソースコードに書くのであれば、モジュールを編集し再コンパイルしなければならない。いずれにしても、この手動プロセスはどちらも遅く間違えやすいし、変更や再設定が行われると次第に混乱を招く。

      Instead of all this hardcoding, you can use resource discovery to let providers and
      consumers of services find one another without needing prior knowledge of the system layout.
      In this section, you’ll build a resource discovery application that functions
      a bit like the yellow pages. Each node in the cluster runs a local instance of this
      application. Each such instance discovers and caches information about the available
      resources in the cluster. This distributed, dynamic approach makes the system flexible
      and powerful for a number of reasons:

全てをハードコーディングする代わりに、システム配置の事前知識なしに サービスの providers と consumers にお互いを見つけさせるための @TODO資源発見 を使うことができる。このセクションでは??イエローページ??に似た資源発見アプリを構築する。クラスター内の各ノードはこのアプリケーション(のローカルインスタンス)を実効する。これらのインスタンスはクラスター内の利用可能な資源を見つけキャッシュする。この分散され動的なアプローチは、いくつかの理由でシステムを柔軟で動的なものにする。

      ■  No single point of failure—It’s a peer-to-peer system.

No SPOF - 相互接続システム

      ■  No hardcoded network topology—You can add resources where you want them.

動的なネットワーク構成 - 好きな場所にリソースを追加できる

      ■  Easier scaling—You can add more resources as needed.

容易なスケーリング - 必要に応じてリソースを追加できる

      ■  Ability to run many services in a single node—Discovery is location transparent and
         works just as well with only one node (particularly good for testing).

単一ノードで多くのサービスを実効できる - 発見は位置透過で単一ノードでも??同様に動作する(テストに特に都合がよい)

      ■  Easier upgrades—You can bring down old services and start new ones dynamically. 
         Removed services become unregistered, and new ones are discovered as they come online.

容易なアップデート - 古いサービスを停止させ、新しいサービスを動的に開始することができる。
削除されたサービスは登録解除され、新しいサービスは接続時に発見される。

      With dynamically discoverable resources, life gets easier all the way from development
      to production (especially in production). Before we move on to implementation, let’s
      first make it clear what we’re talking about.

動的な資源発見により、開発から製品まで幅広く目的の達成(life)が容易になる(特に製品において)。
実装に進む前に、内容についてクリアにしておこう。

8.3.1 Terminology and taxonomy
==============================
8.3.1 専門用語と分類学
==============================

      We need to introduce a few concepts so we can discuss them in a consistent way.
      These concepts are fairly universal, although they may be named differently in other
      literature and implementations. Resource discovery is all about enabling the rela-
      tionship between producers and consumers of resources. For this purpose, it needs
      to track available resources offered by producers, as well as the requirements of con-
      sumers: in other words, a list of “I have” and “I want” for each participant. The “I
      have” items must be concrete resources that can be used or addressed directly,
      whereas the “I want” items only need to indicate the type of resource being sought
      so that a matching resource instance can be found. Table 8.1 shows the terminology
      we use.

      Let’s examine these concepts in a little more detail.

      Table 8.1  Resource discovery terms and definitions
                Term                                            Definition
        Resource               A concrete resource (e.g., a fun) or a reference to a concrete resource
                               (e.g., a pid).
        Resource type          A tag used to classify resources.
        Resource tuple         A pair of a type tag and a resource.

RESOURCE
--------

      A resource is either a specific, concrete resource that you can use directly, such as a fun
      or a chunk of binary data, or a reference to a concrete resource, such as a pid, a file
      handle, an ETS table handle, or similar. In general, you’ll be storing mostly resource
      references in your system, rather than the concrete resources themselves.

RESOURCE TYPE
-------------

      A resource type identifies a certain kind of resource. For example, an instance of
      the Simple Cache application can publish itself as being a resource of type simple_
      cache. There can be many resource instances of the same type in an Erlang cluster,
      and it’s assumed that they all have the same API no matter how they’re imple-
      mented. A consumer that announces that it’s looking for resources of the type
      simple_cache will be told about all resources of this type that have been published
      somewhere in the cluster.

RESOURCE TUPLE
--------------

      A resource tuple is a pair of a resource type and a resource. If you have a resource
      tuple, you have everything you need to know in order to use the resource. The type
      indicates what sort of thing the resource is and how you may access it or talk to it. You
      may publish any resources you have through the discovery system in the form of
      resource tuples, so that anyone who understands the type tag can locate those
      resources and use them.

      With this terminology straightened out, let’s get to the implementation of this sys-
      tem. It isn’t trivial—distributed applications rarely are—but you should be able to fol-
      low what’s going on. We start by explaining the algorithm.

8.3.2 The algorithm
===================
8.3.2 アルゴリズム
===================

      Let’s say you start with two connected nodes, a and b (which are already synchro-
      nized), and now a third node c joins the cluster. The problem you have to solve is how
      to synchronize c with the other nodes. Suppose that both a and b have local resource
      instances of types x and y (we refer to the specific instances as x@a, and so on). Fur-
      thermore, both a and b would like to know about resources of type z. (For example, z
      could be a logging service needed by applications running on a and b.) Node c has a
      local resource of type z, and it’s looking for one or more resources of type x. c doesn’t
      care about resources of type y.

      To get in sync with the other nodes, the resource discovery server on c sends mes-
      sages to a and b informing them what resources it has locally. The resource discovery
      servers on nodes a and b receive these messages and cache the information about the
      resource z@c, which matches their local “I want” lists. They then both respond by
      sending information about their local resources back to c, which caches the informa-
      tion about the resources of type x and discards all information about the resources of
      type y. (You can think of it as a game of “I’ll show you mine, if you show me yours!”)
      Figure 8.6 illustrates these interactions.

      Please make sure you read this explanation carefully before you move on the next
      section. It will be much easier to follow the implementation if you understand the
      algorithm.

      Next, let’s begin implementing the bare-bones resource discovery system, which
      you can take to the moon after you’ve read the rest of this book.
